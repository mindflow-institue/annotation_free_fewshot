{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./model')\n",
    "import dino # model\n",
    "import argparse\n",
    "import utils\n",
    "import os\n",
    "\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncut(feats, dims, scales, init_image_size, tau = 0, eps=1e-5, im_name='', no_binary_graph=False):\n",
    "    \"\"\"\n",
    "    Implementation of NCut Method.\n",
    "    Inputs\n",
    "      feats: the pixel/patche features of an image\n",
    "      dims: dimension of the map from which the features are used\n",
    "      scales: from image to map scale\n",
    "      init_image_size: size of the image\n",
    "      tau: thresold for graph construction\n",
    "      eps: graph edge weight\n",
    "      im_name: image_name\n",
    "      no_binary_graph: ablation study for using similarity score as graph edge weight\n",
    "    \"\"\"\n",
    "    feats = F.normalize(feats, p=2, dim=0)\n",
    "    A = (feats.transpose(0,1) @ feats)\n",
    "    A = A.cpu().numpy()\n",
    "    if no_binary_graph:\n",
    "        A[A<tau] = eps\n",
    "    else:\n",
    "        A = A > tau\n",
    "        A = np.where(A.astype(float) == 0, eps, A)\n",
    "    d_i = np.sum(A, axis=1)\n",
    "    D = np.diag(d_i)\n",
    "\n",
    "    # Print second and third smallest eigenvector\n",
    "    _, eigenvectors = eigh(D-A, D, subset_by_index=[1,2])\n",
    "    eigenvec = np.copy(eigenvectors[:, 0])\n",
    "\n",
    "\n",
    "    # method1 avg\n",
    "    second_smallest_vec = eigenvectors[:, 0]\n",
    "    avg = np.sum(second_smallest_vec) / len(second_smallest_vec)\n",
    "    bipartition = second_smallest_vec > avg\n",
    "\n",
    "    seed = np.argmax(np.abs(second_smallest_vec))\n",
    "\n",
    "    if bipartition[seed] != 1:\n",
    "        eigenvec = eigenvec * -1\n",
    "        bipartition = np.logical_not(bipartition)\n",
    "    bipartition = bipartition.reshape(dims).astype(float)\n",
    "\n",
    "    # predict BBox\n",
    "    pred, _, objects,cc = detect_box(bipartition, seed, dims, scales=scales, initial_im_size=init_image_size) ## We only extract the principal object BBox\n",
    "    mask = np.zeros(dims)\n",
    "    mask[cc[0],cc[1]] = 1\n",
    "\n",
    "    mask = torch.from_numpy(mask).to('cuda')\n",
    "#    mask = torch.from_numpy(bipartition).to('cuda')\n",
    "    bipartition = F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=init_image_size, mode='nearest').squeeze()\n",
    "    \n",
    "\n",
    "    eigvec = second_smallest_vec.reshape(dims) \n",
    "    eigvec = torch.from_numpy(eigvec).to('cuda')\n",
    "    eigvec = F.interpolate(eigvec.unsqueeze(0).unsqueeze(0), size=init_image_size, mode='nearest').squeeze()\n",
    "    return  seed, bipartition.cpu().numpy(), eigvec.cpu().numpy(), eigenvectors\n",
    "\n",
    "def detect_box(bipartition, seed,  dims, initial_im_size=None, scales=None, principle_object=True):\n",
    "    \"\"\"\n",
    "    Extract a box corresponding to the seed patch. Among connected components extract from the affinity matrix, select the one corresponding to the seed patch.\n",
    "    \"\"\"\n",
    "    w_featmap, h_featmap = dims\n",
    "    objects, num_objects = ndimage.label(bipartition)\n",
    "    cc = objects[np.unravel_index(seed, dims)]\n",
    "\n",
    "\n",
    "    if principle_object:\n",
    "        mask = np.where(objects == cc)\n",
    "       # Add +1 because excluded max\n",
    "        ymin, ymax = min(mask[0]), max(mask[0]) + 1\n",
    "        xmin, xmax = min(mask[1]), max(mask[1]) + 1\n",
    "        # Rescale to image size\n",
    "        r_xmin, r_xmax = scales[1] * xmin, scales[1] * xmax\n",
    "        r_ymin, r_ymax = scales[0] * ymin, scales[0] * ymax\n",
    "        pred = [r_xmin, r_ymin, r_xmax, r_ymax]\n",
    "\n",
    "        # Check not out of image size (used when padding)\n",
    "        if initial_im_size:\n",
    "            pred[2] = min(pred[2], initial_im_size[1])\n",
    "            pred[3] = min(pred[3], initial_im_size[0])\n",
    "\n",
    "        # Coordinate predictions for the feature space\n",
    "        # Axis different then in image space\n",
    "        pred_feats = [ymin, xmin, ymax, xmax]\n",
    "\n",
    "        return pred, pred_feats, objects, mask\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation applied to all images\n",
    "ToTensor = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                     (0.229, 0.224, 0.225)),])\n",
    "\n",
    "def get_tokencut_binary_map(img_pth, backbone,patch_size, tau) :\n",
    "    I = Image.open(img_pth).convert('RGB')\n",
    "    I_resize, w, h, feat_w, feat_h = utils.resize_pil(I, patch_size)\n",
    "\n",
    "    tensor = ToTensor(I_resize).unsqueeze(0).cuda()\n",
    "    feat = backbone(tensor)[0]\n",
    "\n",
    "    seed, bipartition, eigvec, eigvectors = ncut(feat, [feat_h, feat_w], [patch_size, patch_size], [h,w], tau)\n",
    "    return bipartition, eigvec, eigvectors.reshape([feat_h, feat_w, 2]).astype(float)\n",
    "\n",
    "def mask_color_compose(org, mask, mask_color = [173, 216, 230]) :\n",
    "\n",
    "    mask_fg = mask > 0.5\n",
    "    rgb = np.copy(org)\n",
    "    rgb[mask_fg] = (rgb[mask_fg] * 0.3 + np.array(mask_color) * 0.7).astype(np.uint8)\n",
    "\n",
    "    return Image.fromarray(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(out_dir='./output', vit_arch='small', vit_feat='k', patch_size=16, tau=0.2, sigma_spatial=16, sigma_luma=16, sigma_chroma=8, dataset=None, nb_vis=100, img_path='D:/deeplearning_Sanaz/TokenCut/TokenCut/examples/mydata/ab_wheel', save_feat_dir='../image')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "## input / output dir\n",
    "parser.add_argument('--out-dir', type=str, default = './output', help='output directory')\n",
    "\n",
    "parser.add_argument('--vit-arch', type=str, default='small', choices=['base', 'small'], help='which architecture')\n",
    "\n",
    "parser.add_argument('--vit-feat', type=str, default='k', choices=['k', 'q', 'v', 'kqv'], help='which features')\n",
    "\n",
    "parser.add_argument('--patch-size', type=int, default=16, choices=[16, 8], help='patch size')\n",
    "\n",
    "parser.add_argument('--tau', type=float, default=0.2, help='Tau for tresholding graph')\n",
    "\n",
    "parser.add_argument('--sigma-spatial', type=float, default=16, help='sigma spatial in the bilateral solver')\n",
    "\n",
    "parser.add_argument('--sigma-luma', type=float, default=16, help='sigma luma in the bilateral solver')\n",
    "\n",
    "parser.add_argument('--sigma-chroma', type=float, default=8, help='sigma chroma in the bilateral solver')\n",
    "\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default=None, choices=['ECSSD', 'DUTS', 'DUT', None], help='which dataset?')\n",
    "\n",
    "parser.add_argument('--nb-vis', type=int, default=100, choices=[1, 200], help='nb of visualization')\n",
    "\n",
    "parser.add_argument('--img-path', type=str, default='fss-dataset/mydata/ab_wheel', help='single image visualization')\n",
    "parser.add_argument('--save_feat_dir',type=str, default= '../image')\n",
    "args = parser.parse_args(args=[])\n",
    "print (args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weight from /dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTFeat(\n",
       "  (model): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define the network for feature extraction\n",
    "if args.vit_arch == 'base' and args.patch_size == 16:\n",
    "    url = \"/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
    "    feat_dim = 768\n",
    "elif args.vit_arch == 'base' and args.patch_size == 8:\n",
    "    url = \"/dino/dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "    feat_dim = 768\n",
    "elif args.vit_arch == 'small' and args.patch_size == 16:\n",
    "    url = \"/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "    feat_dim = 384\n",
    "elif args.vit_arch == 'base' and args.patch_size == 8:\n",
    "    url = \"/dino/dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n",
    "\n",
    "backbone = dino.ViTFeat(url, feat_dim, args.vit_arch, args.vit_feat, args.patch_size)\n",
    "msg = 'Load {} pre-trained feature...'.format(args.vit_arch)\n",
    "# print (msg)\n",
    "backbone.eval()\n",
    "backbone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "from PIL import Image\n",
    "pattern = args.img_path + \"/**/*.jpg\"\n",
    "# Get a list of file paths that match the pattern\n",
    "image_list = glob.glob(pattern, recursive=True)\n",
    "\n",
    "# Iterate over the file paths and load the images using PIL\n",
    "for im_path in image_list:\n",
    "    folder_path,im_pth = os.path.split(im_path)\n",
    "\n",
    "    \n",
    "    if im_pth.endswith('.jpg'):\n",
    "        img = Image.open(im_path)\n",
    "        original_image = imread(im_path)\n",
    "        \n",
    "\n",
    "        im_name =os.path.basename(im_pth)\n",
    "        im_name = im_name.split('.')[0]\n",
    "\n",
    "        \n",
    "        bipartition, eigvec, eigvectors = get_tokencut_binary_map(im_path, backbone, args.patch_size, args.tau)\n",
    "        bipartition = bipartition*255\n",
    "        im_jpg = Image.fromarray( bipartition)\n",
    "        binary_mask = im_jpg.convert('RGB')\n",
    "        binary_mask.save(os.path.join(folder_path,im_name+'_mask'+'.png'))\n",
    "        binary_mask = np.array(binary_mask)\n",
    "        binary_mask = binary_mask.astype(np.float32) / 255\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
